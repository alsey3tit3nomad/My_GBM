# My Educational Implementation of GBM

In this project, I implemented a decision tree, Gradient Boosting Machine (GBM), and several loss functions such as MSE and LogLoss from scratch using numpy
The model was trained and evaluated on a synthetic dataset generated using `sklearn.datasets.make_classification`.
```Python
Accuracy of DecisionTree on training data: 0.9500
Accuracy of My_GBM on training data: 1.0000
```
The goal is to understand how to gradient boosting works under the hood.  

## What I want to add to this project:

1. Optimal split selection in the decision tree
2. Support for more loss functions
3. Unit tests and validation tests for models and loss functions
4. Comparison with sklearn and Catboos
